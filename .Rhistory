import pandas as pd              # package for data wrangling and transformation
import numpy as np               # package for linear algebra
import seaborn as sns            # advanced graphical interface package
import matplotlib.pyplot as plt  # basic graphical interface package
import warnings
warnings.filterwarnings('ignore')
# A. MAIN FILES
# general data (employee attributes dataset)
general_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/general_data.csv')
# B. SURVEY DATA
# 1. manager
manager_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/manager_survey_data.csv')
# 2. employee
employee_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/employee_survey_data.csv')
# C. TIME SERIES DATA (employee - worked time (business hours))
# in time (registered hour (start of the day))
in_time = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/in_time.csv')
# out time (registered hour (end of the day))
out_time = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/out_time.csv')
# D. DICTIONARY OF THE PROJECT
# data dictionary
data_dictionary = pd.read_excel('/home/analytics/R/Projects/Python/datasets/attrition/data_dictionary.xlsx')
import pandas as pd              # package for data wrangling and transformation
import numpy as np               # package for linear algebra
import seaborn as sns            # advanced graphical interface package
import matplotlib.pyplot as plt  # basic graphical interface package
import warnings
warnings.filterwarnings('ignore')
# A. MAIN FILES
# general data (employee attributes dataset)
general_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/general_data.csv')
# B. SURVEY DATA
# 1. manager
manager_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/manager_survey_data.csv')
# 2. employee
employee_data = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/employee_survey_data.csv')
# C. TIME SERIES DATA (employee - worked time (business hours))
# in time (registered hour (start of the day))
in_time = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/in_time.csv')
# out time (registered hour (end of the day))
out_time = pd.read_csv('/home/analytics/R/Projects/Python/datasets/attrition/out_time.csv')
# D. DICTIONARY OF THE PROJECT
# data dictionary
data_dictionary = pd.read_excel('/home/analytics/R/Projects/Python/datasets/attrition/data_dictionary.xlsx')
print(general_data.columns)
# shape of general data - rows / cols of data
print(general_data.shape)
general_data.head(1)
# shape of manager data - rows / cols of data (survey)
print(manager_data.shape)
print(manager_data.head(1))
# shape of employee data - rows / cols of data (survey)
print(employee_data.shape)
print(employee_data.head(1))
df = pd.concat([general_data.set_index('EmployeeID'), manager_data.set_index('EmployeeID'),
employee_data.set_index('EmployeeID')], axis = 1, join = 'inner').reset_index()
df.head(2)
#checking data types of the data
df.dtypes
in_time.head(1)
#data types
in_time.dtypes
in_time.iloc[:, 1:] = in_time.iloc[:, 1:].astype('datetime64[ns]')
in_time.dtypes
# renaming index of in_time data, to employee ID
in_time.rename({'Unnamed: 0': 'EmployeeID'}, axis = 1, inplace = True)
excluded = in_time.isnull().all()
excluded[excluded == True] #all these columns have NULL VALUES, we need to exclude them
excluded_in_cols =list(excluded[excluded == True].index)
excluded_in_cols
#dropping columns that we won't need
in_time.drop(excluded_in_cols, axis=1, inplace=True)
#dropping columns that we won't need
in_time.drop(excluded_in_cols, axis=1, inplace=True)
# Data frame to convert date time into hours (we are going to find the average of hours worked (business hours) )
in_time_hours = pd.DataFrame()
for col in in_time.columns[1:]:
time = pd.DatetimeIndex(in_time[col])  # converts to datetime object index
in_time_hours[col] = (time.hour * 60 + time.minute) / 60
in_time_hours['EmployeeID'] = in_time['EmployeeID']
in_time_hours.head(5)
out_time.iloc[:, 1:] = out_time.iloc[:, 1:].astype('datetime64[ns]')
out_time.rename({'Unnamed: 0': 'EmployeeID'}, axis = 1, inplace = True)
excluded = out_time.isnull().all()
excluded[excluded == True]
excluded_out_cols =list(excluded[excluded == True].index)
out_time.drop(excluded_out_cols, axis=1, inplace=True)
out_time.iloc[:, 1:] = out_time.iloc[:, 1:].astype('datetime64[ns]')
out_time.rename({'Unnamed: 0': 'EmployeeID'}, axis = 1, inplace = True)
out_time_hours = pd.DataFrame()
for col in out_time.columns[1:]:
time = pd.DatetimeIndex(out_time[col])  # converts to datetime object index
out_time_hours[col] = (time.hour * 60 + time.minute) / 60
out_time_hours.head(2)
ts_df = pd.DataFrame()
ts_df['EmployeeID_in'] = in_time['EmployeeID']
ts_df['EmployeeID_out'] = out_time['EmployeeID']
ts_df['EmployeeID'] = ts_df['EmployeeID_in'] - ts_df['EmployeeID_out']
ts_df.loc[ts_df['EmployeeID'] != 0]  #checking if all matched correctly by using our master key: EmployeeID
ts_df.drop(['EmployeeID_in','EmployeeID_out'], axis=1, inplace=True)
ts_df['EmployeeID'] = in_time['EmployeeID']
ts_df.head(2)
ts_df['days_off'] = in_time_hours.isnull().sum(axis = 1)
ts_df.head(2)
attrition = pd.concat([df.set_index('EmployeeID'), ts_df.set_index('EmployeeID')],
axis = 1, join = 'inner').reset_index()
attrition.reset_index(drop = True, inplace = True)
attrition.head(2)
attrition.shape
attrition_null_value_col = attrition.isna().sum()[attrition.isna().sum()!= 0].reset_index().rename(columns=
{'index': 'col_name', 0: '#'})
attrition_null_value_col['%'] = attrition_null_value_col['#'] / attrition.shape[0]
attrition_null_value_col
# NumCompaniesWorked
#We are going to replace the NaN values with "0". This value is not present in the data.
attrition['NumCompaniesWorked'] = attrition['NumCompaniesWorked'].fillna(0)
#TotalWorkingYears
# Again, there is not much information on the dictionary that we can refer too. Also, it doesn't seem to be a pattern on these NaNs. We are going to replace it with 0.
attrition['TotalWorkingYears'] = attrition['TotalWorkingYears'].fillna(0)
# EnvironmentSatisfaction
# No pattern of these NaNs could be detected. There is no much information on the data dictionary. We are going to use the same logic as the previous ones:
attrition['EnvironmentSatisfaction'] = attrition['EnvironmentSatisfaction'].fillna(0)
# JobSatisfaction
# Although these people have several experience in their jobs, the Job Satisfaction was not reflected on the data. Since there is no enough information on the data dictionary, we are going to replace these values using the same logic as before:
attrition['JobSatisfaction'] = attrition['JobSatisfaction'].fillna(0)
# WorkLifeBalance
# No pattern was detected as the previous examples. Let's replace these NaNs with the same logic as before:
attrition['WorkLifeBalance'] = attrition['WorkLifeBalance'].fillna(0)
### CHECKING NANS again:
attrition_null_value_col = attrition.isna().sum()[attrition.isna().sum()!= 0].reset_index().rename(columns=
{'index': 'col_name', 0: '#'})
attrition_null_value_col['%'] = attrition_null_value_col['#'] / attrition.shape[0]
attrition_null_value_col
attrition.select_dtypes(exclude = 'object').apply(pd.Series.nunique, axis = 0)
attrition.drop(['EmployeeCount', 'StandardHours', 'EmployeeID', 'Over18'], axis = 1, inplace = True)
num_feats_1 = list(attrition.select_dtypes(include = ['int64', 'float64'] ).columns)[0:9]
num_feats_2 = list(attrition.select_dtypes(include = ['int64', 'float64'] ).columns)[9:]
fig = plt.figure(figsize=(20,20))
for index, item in enumerate(num_feats_1, 1):
plt.subplot(3, 3, index)
sns.distplot(attrition[attrition.Attrition == 'Yes'][item], color="red", hist = False,  kde=True, norm_hist=True)
sns.distplot(attrition[attrition.Attrition == 'No'][item], color="blue", hist = False,  kde=True, norm_hist=True)
plt.legend(labels=['Yes','No'], title = 'Attrition?')
plt.show()
fig = plt.figure(figsize=(20,20))
for index, item in enumerate(num_feats_2, 1):
plt.subplot(3, 4, index)
sns.distplot(attrition[attrition.Attrition == 'Yes'][item], color="red", hist = False,  kde=True, norm_hist=True)
sns.distplot(attrition[attrition.Attrition == 'No'][item], color="blue", hist = False,  kde=True, norm_hist=True)
plt.legend(labels=['Yes','No'], title = 'Attrition?')
plt.show()
fig = plt.figure(figsize=(20,20))
for index, item in enumerate(num_feats_1, 1):
plt.subplot(3, 3, index)
sns.kdeplot(attrition[attrition.Attrition == 'Yes'][item], color="red", cumulative = True)
sns.kdeplot(attrition[attrition.Attrition == 'No'][item], color="blue", cumulative = True)
plt.legend(labels=['Yes','No'], title = 'Attrition?')
plt.show()
fig = plt.figure(figsize=(20,20))
for index, item in enumerate(num_feats_2, 1):
plt.subplot(3, 4, index)
sns.kdeplot(attrition[attrition.Attrition == 'Yes'][item], color="red", cumulative = True)
sns.kdeplot(attrition[attrition.Attrition == 'No'][item], color="blue", cumulative = True)
plt.legend(labels=['Yes','No'], title = 'Attrition?')
plt.show()
# Storing our categorical features
cat_feats = list(attrition.select_dtypes(include = ['object'] ).columns)
cat_feats
cat_feats = list(attrition.select_dtypes(include = ['object'] ).columns)
def plot_cat(cats = cat_feats, a = 1, b = 1, c = 1 ):
fig = plt.figure(figsize=(15,20))
for i in cats[1:]:
grouped_df = attrition[['Attrition', i ]] .groupby(['Attrition', i]).size().to_frame('Percent')
grouped_df['Percent'] = (grouped_df['Percent'] * 100 / sum(grouped_df['Percent'])).round(0)
grouped_df = grouped_df.reset_index()
plt.subplot(a, b, c)
plt.title('{}, subplot: {}{}{}'.format(i, a, b, c))
plt.xlabel(i)
sns.barplot(x=i, y = 'Percent', hue='Attrition', data=grouped_df)
c = c + 1
plt.show()
plot_cat(cats = cat_feats, a = 3, b = 2, c = 1)
# creating two variables for numerical features and categorical features
numerical_columns = attrition.select_dtypes(exclude = 'object').columns
nmerical_data = attrition[numerical_columns]
categorical_columns = attrition.select_dtypes(include='object').columns
categorical_data = attrition[categorical_columns]
plt.figure(figsize=(20,30))
for index, item in enumerate(numerical_columns, 1):
plt.subplot(8, 3, index)
sns.boxplot(attrition[item])
plt.show()
import copy
attritionv2 = copy.deepcopy(attrition) #creating a copy in our current dataset.
#attrition = attritionv2
cols_to_trans = numerical_columns
for column in cols_to_trans:
try:
attrition[column] = np.log1p(attrition[column]) #applying log(x + 1) transformation
except (ValueError, AttributeError):
pass
dummies = pd.get_dummies(attrition[categorical_columns], drop_first = True)
attrition_dummies = pd.concat([attrition, dummies], axis = 1)
attrition_dummies.drop(categorical_columns, axis = 1, inplace = True)
X = attrition_dummies.drop('Attrition_Yes', axis=1)
y = attrition_dummies['Attrition_Yes']
X.head(2)
y.head()
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
columns_std = X.columns
attrition_dummies[columns_std] = scaler.fit_transform(attrition_dummies[columns_std])
attrition_dummies.head(2)
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif["vars"] = X.columns
vif['vif'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
for index,column in enumerate(X.columns):
if vif['vif'][index]>5:  #revemoving correlated variables above 5 (which is a fixed critical value)
vif = vif.drop([index], axis=0)
# We print the remaining variables
print(vif)
columns_vif = list(vif['vars'])
data = attrition_dummies[columns_vif]
data = pd.concat([data, attrition_dummies['Attrition_Yes']], axis=1)
from sklearn.model_selection import train_test_split
X = data.drop('Attrition_Yes', axis=1)
y = data['Attrition_Yes']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)
#Ploting barplot for target
plt.figure(figsize=(6,4))
g = sns.barplot(data['Attrition_Yes'], data['Attrition_Yes'], palette='Set1', estimator=lambda x: len(x) / len(data) )
#Anotating the graph
for p in g.patches:
width, height = p.get_width(), p.get_height()
x, y = p.get_xy()
g.text(x + width/2,
y + height,
'{:.0%}'.format(height),
horizontalalignment='center',fontsize=15)
#Setting the labels
plt.xlabel('Attrition (0 = No, 1 = Yes)', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.title('Percentage of employees that will/will not ', fontsize=16)
plt.show()
from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, auc, precision_recall_curve, roc_curve, confusion_matrix, make_scorer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, StratifiedKFold, RepeatedStratifiedKFold
## 1. APPLYING LOGISTIC REGRESSION ALGORITHM
log_reg = LogisticRegression()
## 2. Setting the range for class weights (to deal with imbalanced classes)
solvers = ['newton-cg', 'lbfgs', 'liblinear', 'lbfgs']
weights = [{0:x, 1:1.0-x} for x in np.linspace(0.0,0.99,100)]
## 3. Creating a dictionary grid for grid search
param_grid = dict(solver = solvers,
class_weight = weights)
## 4. Selecting accuracy metrics: Precision, Recall and F1 Score
scorers = {
'precision_score': make_scorer(precision_score),
'recall_score': make_scorer(recall_score),
'f1_score':     make_scorer(f1_score)
}
## 5. HYPER PARAMETER TUNNING
gridsearch = GridSearchCV(estimator= log_reg,
param_grid= param_grid,
cv=StratifiedKFold(n_splits = 10),
n_jobs=-1,
scoring=scorers,
refit= 'f1_score',   #we focus on the F1 metric to display the better results
return_train_score=True,
verbose=2).fit(X_train.values, y_train.values)
## 1. APPLYING LOGISTIC REGRESSION ALGORITHM
log_reg = LogisticRegression()
## 2. Setting the range for class weights (to deal with imbalanced classes)
solvers = ['newton-cg', 'lbfgs', 'liblinear', 'lbfgs']
weights = [{0:x, 1:1.0-x} for x in np.linspace(0.0,0.99,100)]
## 3. Creating a dictionary grid for grid search
param_grid = dict(solver = solvers,
class_weight = weights)
## 4. Selecting accuracy metrics: Precision, Recall and F1 Score
scorers = {
'precision_score': make_scorer(precision_score),
'recall_score': make_scorer(recall_score),
'f1_score':     make_scorer(f1_score)
}
## 5. HYPER PARAMETER TUNNING
gridsearch = GridSearchCV(estimator= log_reg,
param_grid= param_grid,
cv=StratifiedKFold(n_splits = 10),
n_jobs=-1,
scoring=scorers,
refit= 'f1_score',   #we focus on the F1 metric to display the better results
return_train_score=True,
verbose=2).fit(X_train.values, y_train.values)
## 1. APPLYING LOGISTIC REGRESSION ALGORITHM
log_reg = LogisticRegression()
## 2. Setting the range for class weights (to deal with imbalanced classes)
solvers = ['newton-cg', 'lbfgs', 'liblinear', 'lbfgs']
weights = [{0:x, 1:1.0-x} for x in np.linspace(0.0,0.99,100)]
## 3. Creating a dictionary grid for grid search
param_grid = dict(solver = solvers,
class_weight = weights)
## 4. Selecting accuracy metrics: Precision, Recall and F1 Score
scorers = {
'precision_score': make_scorer(precision_score),
'recall_score': make_scorer(recall_score),
'f1_score':     make_scorer(f1_score)
}
## 5. HYPER PARAMETER TUNNING
gridsearch = GridSearchCV(estimator= log_reg,
param_grid= param_grid,
cv=StratifiedKFold(n_splits = 10),
n_jobs=-1,
scoring=scorers,
refit= 'f1_score',   #we focus on the F1 metric to display the better results
return_train_score=True,
verbose=2).fit(X_train.values, y_train.values)
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
reticulate::repl_python()
